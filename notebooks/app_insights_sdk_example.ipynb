{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4df43794",
   "metadata": {},
   "source": [
    "# Information Retrieval Simulation Logs\n",
    "\n",
    "This notebook is used with the Application Insights dashboard described in the [readme](../README.md), but you can adapt this approach for other scenarios where synthetic data helps you develop the dashboards and workbooks you need for deep analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c325e2a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will generate fake log events using Faker with the aim of simulating an information retrieval user experience. The log events are intended to be consumed in a Azure Application Insights workbook and dashboard. The user experience is composed of the following user events:\n",
    "\n",
    "- _OnSearch_ - The user submits a query of interest into the system. Additional parameters are `{'query':'<your query string>'}`\n",
    "- _OnResults_ - A list of returned documents by your search engine. Additional parameter for each result is the rank of each document.\n",
    "  A rank in our case is the running index of the returned documents. The first document is ranked 1. `{'indexRank':'<document rank>'}`\n",
    "- _OnNavigate_ - An event where the user navigates to a particular document for further investigation.\n",
    "- _OnSuccess_ - An indication that the document in question was found relative to the search query.\n",
    "\n",
    "<center><img src=\"../images/search_funnel.png\" width=\"70%\" alt=\"user funnel\"/></center>\n",
    "\n",
    "Across all events, we are logging unique identifiers to separate between queries.<br>\n",
    "Each document is identified by a unique `documentId`<br>\n",
    "Each submitted query is identified by a unique `correlationId`<br>\n",
    "The unique identifier is a running `sessionId` + `correlationId`. <br>The `sessionId` is generated automatically by AppInsights. See [User, session, and event analysis in Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/usage-segmentation) for details.<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d68f31d7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Complete the [set up tasks described in the readme](../README.md). You should have an Azure subscription, Azure Application Insights, and an empty dashboard.\n",
    "\n",
    "- Create a `.env` file in the root folder of the sample. Provide the following environment variables. Variables can be used as-is except for the instrumentation key, which you can copy from the Essentials section of the Application Insights Overview page in Azure portal.\n",
    "\n",
    "  ```\n",
    "  API_LOGGING_LEVEL = \"DEBUG\"\n",
    "  LOGGER_NAME = \"test_logger\"\n",
    "  APP_INSIGHTS_KEY = \"<your-app-insights-instrumentation-key>\"\n",
    "  ```\n",
    "\n",
    "- Install the packages listed in the requirements TXT file. Packages include [opencensus-ext-azure](https://pypi.org/project/opencensus-ext-azure/), [python-dotenv](https://pypi.org/project/python-dotenv/), and [Faker](https://pypi.org/project/Faker/).\n",
    "\n",
    "  ```\n",
    "  !pip install -r ../requirements.txt\n",
    "  ```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "daa14fc2",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Your now ready to begin running the code in this notebook. In the following cell, import the libraries used to generate the simulated logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003be9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from src.app_insights_logging import CustomLogger\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94a1e480",
   "metadata": {},
   "source": [
    "## Load Environment Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9bb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# take environment variables from .env.\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# add a custom name for your logger\n",
    "logger_name = os.getenv(\"LOGGER_NAME\")\n",
    "\n",
    "# You can get the instrumentation key in the overview tab in the Azure Portal\n",
    "appinsights_key = os.getenv(\"APP_INSIGHTS_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4911e95b",
   "metadata": {},
   "source": [
    "## Initialize the Azure AppInsights Logger\n",
    "\n",
    "AppInsights allows us to log into separate tables such as traces, events and metrics.<br>\n",
    "In this example we will log into the _traces_ table and the _events_ table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d376220",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_logger = CustomLogger(appinsights_key, logger_name)\n",
    "# This is a custom trace logger in AppInsights\n",
    "trace_logger = custom_logger.setup_azure_trace_logging()\n",
    "\n",
    "# This is a custom event logger in AppInsights\n",
    "event_logger = custom_logger.setup_azure_event_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(event_logger)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb49af8",
   "metadata": {},
   "source": [
    "## Generate user feedback for search results\n",
    "\n",
    "The following code simulates the incidence of feedback (not every user will provide feedback) and the date at which the user session occurred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968d99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "\n",
    "def probability_of_feedback(probability: float = 1 / 3) -> bool:\n",
    "    \"\"\"Returns True if random float < probability threshold, False otherwise\n",
    "\n",
    "    Args:\n",
    "        probability (float, optional): The probability for a user to mark the document as relevant to the search query. Defaults to 1/3.\n",
    "\n",
    "    Returns:\n",
    "        Boolean: True if random float < probability threshold, False otherwise\n",
    "    \"\"\"\n",
    "    return random.random() < probability\n",
    "\n",
    "\n",
    "def get_range_of_dates(start_date: datetime.date, day_count: int) -> list:\n",
    "    \"\"\"\n",
    "        Generate a list of dates starting from `start_date` and sample 70% of the list\n",
    "    Args:\n",
    "        start_date (datetime.date): starting date\n",
    "        day_count (int): Initial date period the generate\n",
    "\n",
    "    Returns:\n",
    "        list: List of dates\n",
    "    \"\"\"\n",
    "    date_range = [\n",
    "        (start_date + datetime.timedelta(days=day)).isoformat()\n",
    "        for day in range(day_count)\n",
    "    ]\n",
    "    return random.sample(date_range, int(day_count * 0.7))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21d01b99",
   "metadata": {},
   "source": [
    "## Generate queries\n",
    "\n",
    "The following code uses [Faker](https://faker.readthedocs.io/en/master/index.html) to generate random three word sentences that substitute for actual queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8c9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "# Number of daily user sessions\n",
    "number_of_sessions_per_timestamp = 3\n",
    "number_of_queries_per_session = 4\n",
    "\n",
    "\n",
    "# The dates we are trying to simulate the logs for\n",
    "# We recommend choosing a date from the recent past\n",
    "start_date = datetime.date(2022, 9, 18)\n",
    "date_range = get_range_of_dates(start_date, 6)\n",
    "\n",
    "# initialize faker\n",
    "faker = Faker()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b0e82c8",
   "metadata": {},
   "source": [
    "## Run the simulation\n",
    "\n",
    "In AppInsights, each event/trace timestamp is logged under the field `timestamp`. For the purposes of this simulation, with the aim of generating a history of searches, we're using the field: `mock_timestamp` as the simulated timestamp field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, random\n",
    "\n",
    "for mock_timestamp in date_range:\n",
    "    print(\"Logging sessions for date:\", mock_timestamp)\n",
    "    mock_timestamp = datetime.datetime.strptime(mock_timestamp, \"%Y-%m-%d\")\n",
    "    for session in range(number_of_sessions_per_timestamp):\n",
    "        # sample unique session_id\n",
    "        session_id = faker.pystr_format()\n",
    "\n",
    "        # sample how many queries were submitted in a single user session\n",
    "        for _ in range(random.randint(1, number_of_queries_per_session)):\n",
    "            correlation_id = faker.pystr_format()\n",
    "\n",
    "            # sample working user\n",
    "            user_id = faker.pystr(6)\n",
    "\n",
    "            # log query event\n",
    "            query = faker.sentence(nb_words=3)[:-1]\n",
    "            query_logging_dimensions = custom_logger.get_logging_dimensions(\n",
    "                event=\"onSearch\",\n",
    "                query=query,\n",
    "                correlationId=correlation_id,\n",
    "                sessionId=session_id,\n",
    "                userId=user_id,\n",
    "                mock_timestamp=str(mock_timestamp),\n",
    "            )\n",
    "\n",
    "            mock_timestamp += datetime.timedelta(seconds=10)\n",
    "            event_logger.info(f\"onSearch\", extra=query_logging_dimensions)\n",
    "            print(\"User submitted query:\", query)\n",
    "\n",
    "            # Log results the user navigates to\n",
    "            for _ in range(random.randint(1, 10)):\n",
    "                index_rank = random.randint(1, 20)\n",
    "                document_id = \"\".join(random.sample(string.ascii_lowercase, 6))\n",
    "                video_logging_dimensions = custom_logger.get_logging_dimensions(\n",
    "                    event=\"onNavigate\",\n",
    "                    query=query,\n",
    "                    correlationId=correlation_id,\n",
    "                    sessionId=session_id,\n",
    "                    userId=user_id,\n",
    "                    indexRank=index_rank,\n",
    "                    documentId=document_id,\n",
    "                    mock_timestamp=str(mock_timestamp),\n",
    "                )\n",
    "                mock_timestamp += datetime.timedelta(seconds=5)\n",
    "                event_logger.info(f\"onNavigate\", extra=video_logging_dimensions)\n",
    "                print(\"Navigating to document:\", document_id)\n",
    "\n",
    "                # If session is successful\n",
    "                if probability_of_feedback(1 / 5):\n",
    "                    feedback_logging_dimensions = custom_logger.get_logging_dimensions(\n",
    "                        event=\"onSuccess\",\n",
    "                        query=query,\n",
    "                        correlationId=correlation_id,\n",
    "                        sessionId=session_id,\n",
    "                        userId=user_id,\n",
    "                        documentId=document_id,\n",
    "                        indexRank=index_rank,\n",
    "                        mock_timestamp=str(mock_timestamp),\n",
    "                    )\n",
    "\n",
    "                    event_logger.info(f\"onSuccess\", extra=feedback_logging_dimensions)\n",
    "                    print(\"Success on document with index:\", index_rank)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6abaf9f2",
   "metadata": {},
   "source": [
    "## View the dashboard in Azure portal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5402ef9",
   "metadata": {},
   "source": [
    "Your data should now be loaded into workbook in Application Insights.\n",
    "\n",
    "1. In Azure portal, in your Application Insights resource, select **Application Dashboard**.\n",
    "\n",
    "   ![Screenshot of the portal page, showing the location of the command at the top of the page.](../images/portal_appinsights_dashboard_cmd.png)\n",
    "\n",
    "1. Select the name of the dashboard you provided for the deployment.\n",
    "\n",
    "   ![Screenshot of the dashboard drop-down list at the top of the page.](../images/portal_appinsight_dashboard_selector.png)\n",
    "\n",
    "1. The dashboard includes metrics that you can't find \"out of the box\" in the portal pages of Azure Cognitive Search. Custom metrics include the _Daily Reciprocal Rate_ and _Successful Over Inspected Rate_ and session metrics.\n",
    "\n",
    "   ![Screenshot of a dashboard containing simulated data.](../images/portal_appinsight_dashboard.png)\n",
    "\n",
    "1. To check the Kusto query that backs each visualization, select **Open Editing Pane** at the top of each tile.\n",
    "\n",
    "   ![Screenshot of the Kusto query and visualization.](../images/portal_appinsight_open_edit_pane.png)\n",
    "\n",
    "1. To use the custom metrics in new Kusto queries, open **Monitoring > Logs** and start a new query that pulls from the CustomEvents table.\n",
    "\n",
    "   ![Screenshot of query editor.](../images/logs_example.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44242b54",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you've seen the dashboard, your next step is to replace simulated data with real-world data from your search application. Revisit the fields used by the dashboard to see which ones your application will need to collect. [Set up diagnostic logging](https://learn.microsoft.com/azure/search/monitor-azure-cognitive-search#enable-resource-logging) for your search service to begin collecting actual logged events in Azure Monitor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ff083f0c83558f9261023d47a77b9b3eb892c62cdbe066d046abcad1a5edb5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
