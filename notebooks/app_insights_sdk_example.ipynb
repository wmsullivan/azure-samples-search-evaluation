{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df43794",
   "metadata": {},
   "source": [
    "# Information Retrieval Simulation Logs\n",
    "\n",
    "This notebook is used with the Application Insights dashboard described in the [readme](../README.md), but you can adapt this approach for other scenarios where synthetic data helps you develop the dashboards and workbooks you need for deep analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c325e2a",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we will generate fake log events using Faker with the aim of simulating an information retrieval user experience. The log events are intended to be consumed in a Azure Application Insights workbook and dashboard. The user experience is composed of the following user events:\n",
    "* *OnSearch* - The user submits a query of interest into the system. Additional parameters are `{'query':'<your query string>'}`\n",
    "* *OnResults* - A list of returned documents by your search engine. Additional parameter for each result is the rank of each document.\n",
    "A rank in our case is the running index of the returned documents. The first document is ranked 1. `{'indexRank':'<document rank>'}`\n",
    "* *OnNavigate* - An event where the user navigates to a particular document for further investigation.\n",
    "* *OnSuccess* - An indication that the document in question was found relative to the search query.\n",
    "\n",
    "<center><img src=\"../images/search_funnel.png\" width=\"70%\" alt=\"user funnel\"/></center> \n",
    "\n",
    "Across all events, we are logging unique identifiers to separate between queries.<br>\n",
    "Each document is identified by a unique `documentId`<br>\n",
    "Each submitted query is identified by a unique `correlationId`<br>\n",
    "The unique identifier is a running `sessionId` + `correlationId`. <br>The `sessionId` is generated automatically by AppInsights. See [User, session, and event analysis in Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/usage-segmentation) for details.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f31d7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Complete the [set up tasks described in the readme](../README.md). You should have an Azure subscription, Azure Application Insights, and an empty dashboard.\n",
    "\n",
    "* Create a `.env` file in the root folder of the sample. Provide the following environment variables. Variables can be used as-is except for the instrumentation key, which you can copy from the Essentials section of the Application Insights Overview page in Azure portal.\n",
    "\n",
    "  ```\n",
    "  API_LOGGING_LEVEL = \"DEBUG\"\n",
    "  LOGGER_NAME = \"test_logger\"\n",
    "  APP_INSIGHTS_KEY = \"<your-app-insights-instrumentation-key>\"\n",
    "  ```\n",
    "\n",
    "* Install the packages listed in the requirements TXT file. Packages include [opencensus-ext-azure](https://pypi.org/project/opencensus-ext-azure/), [python-dotenv](https://pypi.org/project/python-dotenv/), and [Faker](https://pypi.org/project/Faker/).\n",
    "\n",
    "  ```\n",
    "  !pip install -r ../requirements.txt\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14fc2",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "Your now ready to begin running the code in this notebook. In the following cell, import the libraries used to generate the simulated logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003be9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from src.app_insights_logging import CustomLogger\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1e480",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9bb7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# take environment variables from .env.\n",
    "load_dotenv('.env')\n",
    "\n",
    "# add a custom name for your logger\n",
    "logger_name = os.getenv('LOGGER_NAME')\n",
    "\n",
    "# You can get the instrumentation key in the overview tab in the Azure Portal\n",
    "appinsights_key = os.getenv('APP_INSIGHTS_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911e95b",
   "metadata": {},
   "source": [
    "## Initialize the Azure AppInsights Logger\n",
    "AppInsights allows us to log into separate tables such as traces, events and metrics.<br>\n",
    "In this example we will log into the *traces* table and the *events* table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d376220",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_logger = CustomLogger(appinsights_key, logger_name)\n",
    "# This is a custom trace logger in AppInsights\n",
    "trace_logger = custom_logger.setup_azure_trace_logging()\n",
    "\n",
    "# This is a custom event logger in AppInsights\n",
    "event_logger = custom_logger.setup_azure_event_logging() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(event_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb49af8",
   "metadata": {},
   "source": [
    "## Generate user feedback for search results\n",
    "\n",
    "The following code simulates the incidence of feedback (not every user will provide feedback) and the date at which the user session occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968d99ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "def probability_of_feedback(probability: float = 1/3)-> bool:\n",
    "    \"\"\" Returns True if random float < probability threshold, False otherwise\n",
    "\n",
    "    Args:\n",
    "        probability (float, optional): The probability for a user to mark the document as relevant to the search query. Defaults to 1/3.\n",
    "\n",
    "    Returns:\n",
    "        Boolean: True if random float < probability threshold, False otherwise\n",
    "    \"\"\"\n",
    "    return random.random() < probability\n",
    "\n",
    "\n",
    "def get_range_of_dates(start_date: datetime.date, day_count: int) -> list:\n",
    "    \"\"\"\n",
    "        Generate a list of dates starting from `start_date` and sample 70% of the list\n",
    "    Args:\n",
    "        start_date (datetime.date): starting date\n",
    "        day_count (int): Initial date period the generate\n",
    "\n",
    "    Returns:\n",
    "        list: List of dates\n",
    "    \"\"\"\n",
    "    date_range = [(start_date + datetime.timedelta(days = day)).isoformat() for day in range(day_count)]\n",
    "    return random.sample(date_range, int(day_count*0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d01b99",
   "metadata": {},
   "source": [
    "## Generate queries\n",
    "\n",
    "The following code uses [Faker](https://faker.readthedocs.io/en/master/index.html) to generate random three word sentences that substitute for actual queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8c9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "# Number of daily user sessions\n",
    "number_of_sessions_per_timestamp = 3\n",
    "number_of_queries_per_session = 4\n",
    "\n",
    "\n",
    "# The dates we are trying to simulate the logs for\n",
    "# We recommend choosing a date from the recent past\n",
    "start_date = datetime.date(2022, 9, 18)\n",
    "date_range = get_range_of_dates(start_date, 6)\n",
    "\n",
    "# initialize faker\n",
    "faker  = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e82c8",
   "metadata": {},
   "source": [
    "## Run the simulation\n",
    "\n",
    "In AppInsights, each event/trace timestamp is logged under the field `timestamp`. For the purposes of this simulation, with the aim of generating a history of searches, we're using the field: `mock_timestamp` as the simulated timestamp field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, random\n",
    "\n",
    "for mock_timestamp in date_range:\n",
    "    print(\"Logging sessions for date:\" ,mock_timestamp)\n",
    "    mock_timestamp = datetime.datetime.strptime(mock_timestamp, '%Y-%m-%d')\n",
    "    for session in range(number_of_sessions_per_timestamp):\n",
    "\n",
    "        # sample unique session_id\n",
    "        session_id = faker.pystr_format()\n",
    "        \n",
    "        # sample how many queries were submitted in a single user session\n",
    "        for _ in range(random.randint(1, number_of_queries_per_session)):\n",
    "            correlation_id = faker.pystr_format()\n",
    "\n",
    "            # sample working user\n",
    "            user_id = faker.pystr(6)\n",
    "\n",
    "            # log query event\n",
    "            query = faker.sentence(nb_words = 3)[:-1]\n",
    "            query_logging_dimensions = custom_logger.get_logging_dimensions(event=\"onSearch\",\n",
    "                                                                query = query,\n",
    "                                                                correlationId=correlation_id,\n",
    "                                                                sessionId=session_id,\n",
    "                                                                userId=user_id,\n",
    "                                                                mock_timestamp = str(mock_timestamp))\n",
    "            \n",
    "            mock_timestamp += datetime.timedelta(seconds=10)\n",
    "            event_logger.info(f\"onSearch\", extra=query_logging_dimensions)\n",
    "            print('User submitted query:', query)\n",
    "\n",
    "        \n",
    "            # Log results the user navigates to\n",
    "            for _ in range(random.randint(1, 10)):\n",
    "\n",
    "                index_rank = random.randint(1, 20)\n",
    "                document_id = ''.join(random.sample(string.ascii_lowercase, 6))\n",
    "                video_logging_dimensions = custom_logger.get_logging_dimensions(event=\"onNavigate\",\n",
    "                                                                            query = query,\n",
    "                                                                            correlationId=correlation_id,\n",
    "                                                                            sessionId=session_id,\n",
    "                                                                            userId=user_id,\n",
    "                                                                            indexRank = index_rank,\n",
    "                                                                            documentId = document_id,\n",
    "                                                                            mock_timestamp = str(mock_timestamp))\n",
    "                mock_timestamp += datetime.timedelta(seconds=5)\n",
    "                event_logger.info(f\"onNavigate\", extra=video_logging_dimensions)\n",
    "                print('Navigating to document:', document_id)\n",
    "\n",
    "                # If session is successful\n",
    "                if probability_of_feedback(1/5):     \n",
    "                    feedback_logging_dimensions = custom_logger.get_logging_dimensions(\n",
    "                                                        event=\"onSuccess\",\n",
    "                                                        query = query,\n",
    "                                                        correlationId=correlation_id,\n",
    "                                                        sessionId=session_id,\n",
    "                                                        userId=user_id,\n",
    "                                                        documentId = document_id,\n",
    "                                                        indexRank = index_rank,\n",
    "                                                        mock_timestamp = str(mock_timestamp)\n",
    "                                                        )   \n",
    "                    \n",
    "                    event_logger.info(f\"onSuccess\", extra=feedback_logging_dimensions)\n",
    "                    print('Success on document with index:', index_rank)\n",
    "\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abaf9f2",
   "metadata": {},
   "source": [
    "## View the dashboard in Azure portal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5402ef9",
   "metadata": {},
   "source": [
    "\n",
    "Your data should now be loaded into workbook in Application Insights. \n",
    "\n",
    "1. In Azure portal, in your Application Insights resource, select **Application Dashboard**.\n",
    "\n",
    "   ![Screenshot of the portal page, showing the location of the command at the top of the page.](../images/portal_appinsights_dashboard_cmd.png)\n",
    "\n",
    "1. Select the name of the dashboard you provided for the deployment.\n",
    "\n",
    "   ![Screenshot of the dashboard drop-down list at the top of the page.](../images/portal_appinsight_dashboard_selector.png)\n",
    "\n",
    "1. The dashboard includes metrics that you can't find \"out of the box\" in the portal pages of Azure Cognitive Search. Custom metrics include the *Daily Reciprocal Rate* and *Successful Over Inspected Rate* and session metrics. \n",
    "\n",
    "   ![Screenshot of a dashboard containing simulated data.](../images/portal_appinsight_dashboard.png)\n",
    "\n",
    "1. To check the Kusto query that backs each visualization, select **Open Editing Pane** at the top of each tile.\n",
    "\n",
    "   ![Screenshot of the Kusto query and visualization.](../images/portal_appinsight_open_edit_pane.png)\n",
    "\n",
    "1. To use the custom metrics in new Kusto queries, open **Monitoring > Logs** and start a new query that pulls from the CustomEvents table.\n",
    "\n",
    "   ![Screenshot of query editor.](../images/logs_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44242b54",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that you've seen the dashboard, your next step is to replace simulated data with real-world data from your search application. Revisit the fields used by the dashboard to see which ones your application will need to collect. [Set up diagnostic logging](https://learn.microsoft.com/azure/search/monitor-azure-cognitive-search#enable-resource-logging) for your search service to begin collecting actual logged events in Azure Monitor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9ff083f0c83558f9261023d47a77b9b3eb892c62cdbe066d046abcad1a5edb5c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
